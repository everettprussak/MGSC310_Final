---
title: "Final Project Proposal"
author: "Hudson Pak, Justin Lewinski, Everett Prussak"
subtitle: MGSC 310 Final Projet Proposal
output:
  html_document:
    df_print: paged
  html_notebook: default
---

```{r setup, include=FALSE}


library(knitr)

# As long as you are working in a Rstudio Project file, you shouldn't need to 'hard code' directories like this 
# change to your own working directory
#knitr::opts_knit$set(root.dir = 'C:/Users/doosti/Desktop/MGSC_310')

# set seed to your own favorite number
set.seed(310)
options(width=70)
# if you want to prevent scientific format for numbers use this:
options(scipen=99)

# general rchunk code options
opts_chunk$set(tidy.opts=list(width.wrap=50),tidy=FALSE, size = "vsmall")
opts_chunk$set(message = FALSE,
               warning = FALSE,
               cache = TRUE,
               autodep = TRUE,
               cache.comments = FALSE,
               collapse = TRUE,
               fig.width = 5,  
               fig.height = 4,
               fig.align='center')

```

```{r setup_2}

# load all your libraries here
library('tidyverse')
library(ggcorrplot)
library(rsample)
library(plotROC)
library(yardstick)
library(glmnet)
library(glmnetUtils)
library(Metrics)
# note, do not run install.packages() inside a code chunk. install them in the console outside of a code chunk. 

```




Summary Statistics

```{r}

# code
data <- read.csv('diabetes.csv')
data <- data[-7]
data <- data %>%
  mutate(bp_st = BloodPressure*SkinThickness,
         age_squared = Age * Age
         )

head(data)
summary(data)
```
age_squared and bp_st are created because the original model was slightly underfit. By adding these created variables, the model became mode complex and allowed for the model to be less underfit.

Train-Test-Split

```{r}

# code
library(rsample)
set.seed(310)
data_split <- initial_split(data,prop=0.8)
data_train <- training(data_split)
data_test <- testing(data_split)


```



Logistic Regression Model
```{r}
logit <- glm(Outcome ~ .,
             data = data_train,
             family = binomial
             )

summary(logit)
```
At a 95% confidence Glucose, BloodPressure, BMI, Age, and age_squared are statistically significant.

Exponentiating Coefficients
```{r}
exp(logit$coefficients)

```
Interpretation for the variable 'Glucose' (Marginal Effect):
he marginal effect of Glucose shows us that 1 unit increase in Glucose would result in 3.39 percent increase in likelihood of having diabetes.


Scoring the model:
```{r}

results_train <- data.frame(
  true_numeric = data_train$Outcome,
  true_factor = factor(data_train$Outcome),
  scores = predict(logit, type = 'response', newdata = data_train)
)

results_test <- data.frame(
  true_numeric = data_test$Outcome,
  true_factor = factor(data_test$Outcome),
  scores = predict(logit, type = 'response', newdata = data_test)
)

```


ROC Curves:
```{r}
roc_train <- ggplot(results_train, aes(m=scores, d=true_numeric)) + 
  geom_roc(cutoffs.at = c(0.7,0.6,0.5,0.4,0.3)) +
  labs("ROC Curve (Train)", x = "False Positive Rate", y ="True Positive Rate")

print(roc_train)

roc_test <- ggplot(results_test, aes(m=scores, d=true_numeric)) + 
  geom_roc(cutoffs.at = c(0.7,0.6,0.5,0.4,0.3)) +
  labs("ROC Curve (Test)", x = "False Positive Rate", y ="True Positive Rate")

print(roc_test)
```

From these two graph's, we chose a cutoff value of 0.3 because it seems to be a good cutoff as it results in balanced false and true positive rates.


Class Assignments
```{r}
predicted_train <- ifelse(results_train$scores>0.3,1,0)
results_train <- results_train %>% 
  mutate(predicted = factor(predicted_train))

results_train %>% glimpse()

predicted_test <- ifelse(results_test$scores>0.3,1,0)
results_test <- results_test %>% 
  mutate(predicted = factor(predicted_test))

results_test %>% glimpse()
```

Train Confusion Matrix
```{r}
cm_train <- conf_mat(results_train, truth = true_factor, estimate = predicted)
print(cm_train)
```
Train Accuracy, Sensitivity, and Specificity Scores:
```{r}
accuracy <- (288+172) / (172+288+40+114)
paste0('Accuracy for the train is ', round(accuracy,4)*100, ' percent')

sensitivity <- (172) / (172 + 40)
paste0('Sensitivity for the train is ', round(sensitivity,4)*100, ' percent')

specificity <- (288) / (288 + 114)
paste0('Specificity for the train is ', round(specificity,4)*100, ' percent')

```


Test Confusion Matrix
```{r}
cm_test <- conf_mat(results_test, truth = true_factor, estimate = predicted)
print(cm_test)
```

Test Accuracy, Sensitivity, and Specificity Scores:
```{r}
accuracy <- (67+51) / (67+51+31+5)
paste0('Accuracy for the test is ', round(accuracy,4)*100, ' percent')

sensitivity <- (51) / (51+5)
paste0('Sensitivity for the test is ', round(sensitivity,4)*100, ' percent')

specificity <- (67) / (67 + 31)
paste0('Specificity for the test is ', round(specificity,4)*100, ' percent')

```


AUC
```{r}
calc_auc(roc_train)
calc_auc(roc_test)
```

Interestingly, we see that the AUC for the test-set is actually slightly better than the train data.



Lasso Model
```{r}
lasso_mod <- cv.glmnet(Outcome ~ .,
                       data = data_train,
                       # note alpha = 1 sets Lasso!  
                       alpha = 1)

plot(lasso_mod)
print(lasso_mod$lambda.min)

print(lasso_mod$lambda.1se)

coef(lasso_mod, s = lasso_mod$lambda.min) %>% 
  round(3)


coef(lasso_mod, s = lasso_mod$lambda.1se) %>% 
  round(3)

lasso_coefs <- data.frame(
  lasso_min = coef(lasso_mod, s = lasso_mod$lambda.min) %>% 
    round(3) %>% as.matrix() ,
  lasso_1se = coef(lasso_mod, s = lasso_mod$lambda.1se) %>% 
    round(3) %>% as.matrix() 
) %>% rename(lasso_min = 1, lasso_1se = 2)

print(lasso_coefs)

library('coefplot')
coefpath(lasso_mod)

# number of non-zero predictors
lasso_coefs %>% head()
# for lambda.min
lasso_coefs %>% filter(lasso_min != 0) %>% 
  nrow()

    

# for lambda.1se
lasso_coefs %>% filter(lasso_1se != 0) %>% 
  nrow()

```

Performance of Lasso

```{r}
library(caret)
predict_train_1 <- predict(lasso_mod, s=lasso_mod$lambda.min,data_train)

predict_test_1 <- predict(lasso_mod,s = lasso_mod$lambda.min, data_test)

results_train_1 <- data_train %>% mutate(pred = predict_train_1)
RMSE(results_train_1$pred,results_train_1$Outcome)
# test
results_test_1 <- data_test %>% mutate(pred = predict_test_1)
RMSE(results_test_1$pred,results_test_1$Outcome)


```



