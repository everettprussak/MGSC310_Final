---
title: "Final Project Proposal"
author: "Hudson Pak, Justin Lewinski, Everett Prussak"
subtitle: MGSC 310 Final Projet Proposal
output:
  html_document:
    df_print: paged
  html_notebook: default
---

```{r setup, include=FALSE}


library(knitr)

# As long as you are working in a Rstudio Project file, you shouldn't need to 'hard code' directories like this 
# change to your own working directory
#knitr::opts_knit$set(root.dir = 'C:/Users/doosti/Desktop/MGSC_310')

# set seed to your own favorite number
set.seed(310)
options(width=70)
# if you want to prevent scientific format for numbers use this:
options(scipen=99)

# general rchunk code options
opts_chunk$set(tidy.opts=list(width.wrap=50),tidy=FALSE, size = "vsmall")
opts_chunk$set(message = FALSE,
               warning = FALSE,
               cache = TRUE,
               autodep = TRUE,
               cache.comments = FALSE,
               collapse = TRUE,
               fig.width = 5,  
               fig.height = 4,
               fig.align='center')

```

```{r setup_2}

# load all your libraries here
library('tidyverse')
library(ggcorrplot)
library(rsample)
library(plotROC)
library(yardstick)
library(glmnet)
library(glmnetUtils)
library(Metrics)
# note, do not run install.packages() inside a code chunk. install them in the console outside of a code chunk. 

```




Summary Statistics

```{r}

# code
data <- read.csv('diabetes.csv')
#data <- data[-7]
data <- data %>%
  mutate(bp_st = BloodPressure*SkinThickness,
         age_squared = Age * Age,
         glucose_insulin = Glucose * Insulin,
         insulin_squared = Insulin * Insulin,
         Outcome = factor(Outcome)
         )

dim(data)
data <- data %>% filter(
  Glucose > 0,
  BloodPressure > 0,
  SkinThickness > 0,
  BMI > 0
)

head(data)
summary(data)

dim(data)
```
age_squared and bp_st are created because the original model was slightly underfit. By adding these created variables, the model became mode complex and allowed for the model to be less underfit.

Train-Test-Split

```{r}

# code
library(rsample)
set.seed(300)
data_split <- initial_split(data,prop=0.8)
data_train <- training(data_split)
data_test <- testing(data_split)


```



Logistic Regression Model
```{r}
logit <- glm(Outcome ~ .,
             data = data_train,
             family = binomial
             )

summary(logit)
```
At a 95% confidence Glucose, BloodPressure, BMI, DiabetesPedigreeFunction, Age, and age_squared are statistically significant.

Exponentiating Coefficients
```{r}
exp(logit$coefficients)

```
Interpretation for the variable 'Glucose' (Marginal Effect):
he marginal effect of Glucose shows us that 1 unit increase in Glucose would result in 3.38 percent increase in likelihood of having diabetes.


Scoring the model:
```{r}

results_train <- data.frame(
  true_numeric = as.numeric(data_train$Outcome),
  true_factor = data_train$Outcome,
  scores = predict(logit, type = 'response', newdata = data_train)
)

results_test <- data.frame(
  true_numeric = as.numeric(data_test$Outcome),
  true_factor = data_test$Outcome,
  scores = predict(logit, type = 'response', newdata = data_test)
)

```


ROC Curves:
```{r}
roc_train <- ggplot(results_train, aes(m=scores, d=true_numeric)) + 
  geom_roc(cutoffs.at = c(0.7,0.6,0.5,0.4,0.3)) +
  labs("ROC Curve (Train)", x = "False Positive Rate", y ="True Positive Rate")

print(roc_train)

roc_test <- ggplot(results_test, aes(m=scores, d=true_numeric)) + 
  geom_roc(cutoffs.at = c(0.7,0.6,0.5,0.4,0.3)) +
  labs("ROC Curve (Test)", x = "False Positive Rate", y ="True Positive Rate")

print(roc_test)
```

From these two graph's, we chose a cutoff value of 0.3 because it seems to be a good cutoff as it results in balanced false and true positive rates.


Class Assignments
```{r}
predicted_train <- ifelse(results_train$scores>0.3,1,0)
results_train <- results_train %>% 
  mutate(predicted = factor(predicted_train))

results_train %>% glimpse()

predicted_test <- ifelse(results_test$scores>0.3,1,0)
results_test <- results_test %>% 
  mutate(predicted = factor(predicted_test))

results_test %>% glimpse()
```

Train Confusion Matrix
```{r}
train_cm <- conf_mat(results_train, truth = true_factor, estimate = predicted)
print(train_cm)

```
Train Accuracy, Sensitivity, and Specificity Scores:
```{r}
accuracy <- (207+124) / (207+124+23+71)
paste0('Accuracy for the train is ', round(accuracy,4)*100, ' percent')

sensitivity <- (124) / (124 + 23)
paste0('Sensitivity for the train is ', round(sensitivity,4)*100, ' percent')

specificity <- (207) / (207 + 71)
paste0('Specificity for the train is ', round(specificity,4)*100, ' percent')

```

Test Confusion Matrix
```{r}
cm_test <- conf_mat(results_test, truth = true_factor, estimate = predicted)
print(cm_test)
```


Test Accuracy, Sensitivity, and Specificity Scores:
```{r}
accuracy <- (62+22) / (22+62+8+15)
paste0('Accuracy for the test is ', round(accuracy,4)*100, ' percent')

sensitivity <- (22) / (22+8)
paste0('Sensitivity for the test is ', round(sensitivity,4)*100, ' percent')

specificity <- (62) / (62 + 15)
paste0('Specificity for the test is ', round(specificity,4)*100, ' percent')

```


AUC
```{r}
calc_auc(roc_train)
calc_auc(roc_test)




```

The AUC for the training data is 0.87 and AUC for the testing data is 0.84. These numbers are close to each, but can show a slight overfitting. 

Decision Tree Model

```{r}
library(tree)
tree_diabetes <- tree(Outcome ~.,
                 data = data_train)
              #   control = tree.control(nobs=nrow(data_train),
                      #                  mindev = 0.005))

summary(tree_diabetes)
```

```{r}
plot(tree_diabetes)
text(tree_diabetes,pretty=0, cex = 0.75)
```


Pruning Tree
```{r}
cv.diabetes <- cv.tree(tree_diabetes, FUN=prune.misclass)

plot(cv.diabetes, type='b')

cv_results <- data.frame(
  tree_size = cv.diabetes$size,
  error = cv.diabetes$dev
)

print(cv_results)

pruned_tree <- prune.tree(tree_diabetes, best = 10)
```

Pruned Tree Plot
```{r}
# plot the pruned tree
plot(pruned_tree)
text(pruned_tree, pretty=0, cex = 0.75)
```

Train Results

```{r}

results_train_2 <- data.frame(
  true_numeric = as.numeric(data_train$Outcome),
  true_factor = data_train$Outcome,
  predict = predict(pruned_tree, type = 'class', newdata = data_train)
)

library(yardstick)
cm_train_2 <- conf_mat(results_train_2, truth = true_factor, estimate = predict)
print(cm_train_2)



accuracy <- (230+115) / (230+115+48+32)
paste0('Accuracy for the test is ', round(accuracy,4)*100, ' percent')

sensitivity <- (115) / (115+32)
paste0('Sensitivity for the test is ', round(sensitivity,4)*100, ' percent')

specificity <- (230) / (230+48)
paste0('Specificity for the test is ', round(specificity,4)*100, ' percent')

```

Test Results
```{r}
results_test_2 <- data.frame(
  true_numeric = as.numeric(data_test$Outcome),
  true_factor = data_test$Outcome,
  predict = predict(pruned_tree, type = 'class', newdata = data_test)
)

library(yardstick)
cm_test_2 <- conf_mat(results_test_2, truth = true_factor, estimate = predict)
print(cm_test_2)

accuracy <- (60+20) / (20+60+17+10)
paste0('Accuracy for the test is ', round(accuracy,4)*100, ' percent')

sensitivity <- (20) / (20+10)
paste0('Sensitivity for the test is ', round(sensitivity,4)*100, ' percent')

specificity <- (60) / (60 + 17)
paste0('Specificity for the test is ', round(specificity,4)*100, ' percent')

```



AUC Scores
```{r}
library(pROC)

train_auc <- roc(results_train_2$true_numeric, as.numeric(results_train_2$predict))
print("Train AUC")
print(train_auc)


test_auc <- roc(results_test_2$true_numeric, as.numeric(results_test_2$predict))

print("Test AUC")
print(test_auc)



```



